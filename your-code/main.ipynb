{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5896/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "cannot instantiate 'WindowsPath' on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_training \u001b[38;5;241m=\u001b[39m \u001b[43mWindowsPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Salva/Documents/AI Engeneering Ironhack/Week 4/lab-natural-language-processing/data/kg_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## Read Data for the Fraudulent Email Kaggle Challenge\u001b[39;00m\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path_training,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/pathlib.py:873\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_parts(args)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: cannot instantiate 'WindowsPath' on your system"
     ]
    }
   ],
   "source": [
    "path_training = WindowsPath(\"C:/Users/Salva/Documents/AI Engeneering Ironhack/Week 4/lab-natural-language-processing/data/kg_train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(path_training,encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 2)\n",
      "Validation set shape: (200, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   text  label\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1\n",
       "535  I have not been able to reach oscar this am. W...      0\n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0\n",
       "557  I can have it announced here on Monday - can't...      0\n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1\n",
       "..                                                 ...    ...\n",
       "106  7653 2612ADAMA IBRAHIM________________________...      1\n",
       "270             What does that mean for our schedules?      0\n",
       "860  Dear Friend,My Compliment to you,I guess this ...      1\n",
       "435  Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...      1\n",
       "102  Let me know if today or tomorrow works for you...      0\n",
       "\n",
       "[800 rows x 2 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `data` contains the reduced training set (1000 rows)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes to confirm the split\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", val_data.shape)\n",
    "train_data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):  # Handle missing values (NaN)\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Remove HTML comments\n",
    "    comments = soup.findAll(text=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "\n",
    "    # Extract text and return cleaned content\n",
    "    return soup.get_text(separator=\" \")\n",
    "\n",
    "# Apply BeautifulSoup to the entire dataset\n",
    "def clean_dataset(dataset, text_column):\n",
    "    dataset[text_column] = dataset[text_column].apply(clean_html)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salva\\AppData\\Local\\Temp\\ipykernel_1504\\1710886297.py:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comments = soup.findAll(text=lambda text: isinstance(text, Comment))\n",
      "C:\\Users\\Salva\\AppData\\Local\\Temp\\ipykernel_1504\\1710886297.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1\n",
      "535  I have not been able to reach oscar this am. W...      0\n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0\n",
      "557  I can have it announced here on Monday - can't...      0\n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1\n",
      "                                                  text  label\n",
      "521  Dear Sir=2C I wish you go through this offer t...      1\n",
      "737  To take your mind off the Balkans for a second...      0\n",
      "740                       Pls keep the updates coming!      0\n",
      "660  CHRIST BETHEL HOSPITAL 11 RUE ABOBOTE,ABIDJAN ...      1\n",
      "411  sbwhoeopFriday February 5 2010 7:11 AMHRe: Bra...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salva\\AppData\\Local\\Temp\\ipykernel_1504\\1710886297.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Use the column name 'text' instead of a numerical index\n",
    "text_column = 'text'  # Column label for the text data\n",
    "\n",
    "# Clean the training and validation sets\n",
    "train_data_cleaned = clean_dataset(train_data, text_column)\n",
    "val_data_cleaned = clean_dataset(val_data, text_column)\n",
    "\n",
    "# Display the cleaned data to check\n",
    "print(train_data_cleaned.head())\n",
    "print(val_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to string to avoid errors\n",
    "    processed_feature = str(text)\n",
    "\n",
    "    # Remove all special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    processed_feature = re.sub(r'\\b\\w\\b', '', processed_feature)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'^\\s*[a-zA-Z]\\s+', '', processed_feature)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature).strip()\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    processed_feature = re.sub(r\"^b\\s+\", '', processed_feature)\n",
    "    \n",
    "    #Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "    return processed_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "29   regards mr nelson smith kindly reply me on my ...      1\n",
      "535  have not been able to reach oscar this am we a...      0\n",
      "695  huma abedin b6i checking with pat on the 50k w...      0\n",
      "557     can have it announced here on monday can today      0\n",
      "836  bank of africaagence san pedro14 bp 1210 san p...      1\n",
      "                                                  text  label\n",
      "521  dear sir 2c wish you go through this offer to ...      1\n",
      "737  to take your mind off the balkans for second s...      0\n",
      "740                        pls keep the updates coming      0\n",
      "660  christ bethel hospital 11 rue abobote abidjan ...      1\n",
      "411  sbwhoeopfriday february 2010 11 amhre bravo br...      0\n"
     ]
    }
   ],
   "source": [
    "# Clean the training and validation sets\n",
    "train_data_cleaned = train_data.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "val_data_cleaned = val_data.copy()\n",
    "\n",
    "# Apply the clean_text function to the 'text' column\n",
    "train_data_cleaned[text_column] = train_data_cleaned[text_column].apply(clean_text)\n",
    "val_data_cleaned[text_column] = val_data_cleaned[text_column].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data to check\n",
    "print(train_data_cleaned.head())\n",
    "print(val_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Join the remaining words back into a string\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[1;32m---> 15\u001b[0m train_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_cleaned\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n\u001b[0;32m     16\u001b[0m val_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Display the cleaned data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# Define stop words set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the remaining words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "train_data_cleaned['text'] = train_data_cleaned['text'].apply(remove_stopwords)\n",
    "val_data_cleaned['text'] = val_data_cleaned['text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(train_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Tokenize the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "29   regard mr nelson smith kindly reply private em...      1\n",
      "535      able reach oscar supposed send pdb 11 receive      0\n",
      "695  huma abedin b6i checking pat 50k work jack jak...      0\n",
      "557                             announced monday today      0\n",
      "836  bank africaagence san pedro14 bp 1210 san pedr...      1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply lemmatization to your DataFrame\n",
    "train_data_cleaned['text'] = train_data_cleaned['text'].apply(lemmatize_text)\n",
    "val_data_cleaned['text'] = val_data_cleaned['text'].apply(lemmatize_text)\n",
    "\n",
    "# Display the cleaned and lemmatized data\n",
    "print(train_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some things were not cleaned properly so here is another go at it. \n",
    "\n",
    "\n",
    "def enhanced_clean_text(text):\n",
    "    # Step 1: Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Step 2: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 3: Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Step 4: Fix boundary issues (e.g., missing spaces between lowercase and uppercase letters)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Step 5: Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 6: Remove prefixed 'b' if present\n",
    "    text = re.sub(r\"^b'\", '', text)\n",
    "    \n",
    "    # Step 7: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Reapply the updated cleaning function\n",
    "#train_data_cleaned['text'] = train_data_cleaned['text'].apply(enhanced_clean_text)\n",
    "#val_data_cleaned['text'] = val_data_cleaned['text'].apply(enhanced_clean_text)\n",
    "\n",
    "# Display the updated cleaned data\n",
    "#print(train_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate spam and ham messages\n",
    "spam_messages = train_data[train_data['label'] == 1]['text']\n",
    "ham_messages = train_data[train_data['label'] == 0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in spam messages: [('the', 5769), ('to', 4655), ('of', 4097), ('and', 3339), ('you', 2713), ('in', 2686), ('this', 2239), ('my', 1794), ('your', 1753), ('for', 1710)]\n",
      "Top 10 words in ham messages: [('the', 1558), ('to', 933), ('and', 737), ('of', 694), ('in', 542), ('that', 367), ('for', 327), ('is', 327), ('on', 282), ('you', 255)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get top N words from a set of messages\n",
    "def get_top_n_words(messages, n=10):\n",
    "    vectorizer = CountVectorizer()\n",
    "    word_count_vector = vectorizer.fit_transform(messages)\n",
    "    \n",
    "    # Sum up word counts for each word\n",
    "    word_count_sum = word_count_vector.sum(axis=0)\n",
    "    \n",
    "    # Create a dictionary mapping words to their frequency\n",
    "    word_freq = [(word, word_count_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    # Sort the dictionary by frequency and get top n words\n",
    "    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top n words\n",
    "    return word_freq[:n]\n",
    "\n",
    "# Get top 10 words for spam and ham messages\n",
    "top_10_spam_words = get_top_n_words(spam_messages)\n",
    "top_10_ham_words = get_top_n_words(ham_messages)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words in spam messages:\", top_10_spam_words)\n",
    "print(\"Top 10 words in ham messages:\", top_10_ham_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n"
     ]
    }
   ],
   "source": [
    "# Load the original data\n",
    "path_training = WindowsPath(\"C:/Users/Salva/Documents/AI Engeneering Ironhack/Week 4/lab-natural-language-processing/data/kg_train.csv\")\n",
    "data = pd.read_csv(path_training, encoding='latin-1')\n",
    "\n",
    "# Define extra feature lists\n",
    "money_simbol_list = r\"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = r\"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \n",
    "                              \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "\n",
    "\n",
    "# Display the updated dataset with the new features\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salva\\AppData\\Local\\Temp\\ipykernel_28260\\1710886297.py:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comments = soup.findAll(text=lambda text: isinstance(text, Comment))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 6)\n",
      "Validation set shape: (200, 6)\n"
     ]
    }
   ],
   "source": [
    "# Reduce the data (optional step, if necessary)\n",
    "data = data.head(1000)  # Reduce to 1000 rows, for example\n",
    "data['preprocessed_text'] = data['text'].apply(clean_text)\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(clean_html) \n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(remove_stopwords)\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(lemmatize_text)\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(enhanced_clean_text)\n",
    "\n",
    "\n",
    "# Add the extra features to the original dataset\n",
    "data['money_mark'] = data['preprocessed_text'].str.contains(money_simbol_list).astype(int)\n",
    "data['suspicious_words'] = data['preprocessed_text'].str.contains(suspicious_words).astype(int)\n",
    "data['text_len'] = data['preprocessed_text'].apply(len)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes to confirm the split\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", val_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                     preprocessed_text  money_mark  \\\n",
      "29   regard mr nelson smith kindly reply private em...           1   \n",
      "535         able reach oscar supposed send pdb receive           1   \n",
      "695  huma abedin bi checking pat work jack jake res...           1   \n",
      "557                             announced monday today           1   \n",
      "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        79  \n",
      "535                 0        42  \n",
      "695                 0        79  \n",
      "557                 0        22  \n",
      "836                 1      1063  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply lemmatization to your DataFrame\n",
    "#train_data['preprocessed_text'] = train_data['preprocessed_text'].apply(lemmatize_text)\n",
    "#val_data['preprocessed_text'] = val_data['preprocessed_text'].apply(lemmatize_text)\n",
    "\n",
    "# Display the cleaned and lemmatized data\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words matrix: (1000, 21989)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming your dataframe is 'data' with a 'preprocessed_text' column and new features\n",
    "preprocessed_texts = data['preprocessed_text']\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text into a Bag of Words sparse matrix\n",
    "X_bow = vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Check the shape of the Bag of Words matrix\n",
    "print(\"Shape of Bag of Words matrix:\", X_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (1000, 21989)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'preprocessed_text' column contains the text you want to vectorize\n",
    "texts = data['preprocessed_text']\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the entire dataset into TF-IDF representation\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Check the shape of the resulting TF-IDF matrix\n",
    "print(\"Shape of the TF-IDF matrix:\", X_tfidf.shape)\n",
    "\n",
    "# Optional: Convert the sparse matrix to a dense matrix (memory-heavy)\n",
    "# X_tfidf_dense = X_tfidf.toarray()\n",
    "# print(X_tfidf_dense[:5])  # First 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined feature matrix: (1000, 21992)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Step 1: Extract the new features as a DataFrame\n",
    "new_features = data[['money_mark', 'suspicious_words', 'text_len']]\n",
    "\n",
    "# Step 2: Convert the dense new features DataFrame into a sparse matrix\n",
    "new_features_sparse = csr_matrix(new_features.values)\n",
    "\n",
    "# Step 3: Combine the Bag of Words matrix with new features using hstack\n",
    "X_combined = hstack([X_bow, new_features_sparse])\n",
    "\n",
    "# Step 4: Check the shape of the combined feature matrix\n",
    "print(\"Shape of the combined feature matrix:\", X_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in spam messages: [('br', 1103), ('money', 987), ('account', 898), ('bank', 801), ('fund', 780), ('transaction', 555), ('business', 514), ('country', 513), ('mr', 482), ('nbsp', 475)]\n",
      "Top 10 words in ham messages: [('state', 169), ('pm', 127), ('would', 107), ('president', 99), ('time', 95), ('call', 94), ('mr', 91), ('obama', 84), ('percent', 81), ('secretary', 77)]\n"
     ]
    }
   ],
   "source": [
    "# Function to get top N words from the preprocessed text\n",
    "def get_top_n_words(messages, n=10):\n",
    "    vectorizer = CountVectorizer()\n",
    "    word_count_vector = vectorizer.fit_transform(messages)\n",
    "    \n",
    "    # Sum up word counts for each word\n",
    "    word_count_sum = word_count_vector.sum(axis=0)\n",
    "    \n",
    "    # Create a dictionary mapping words to their frequency\n",
    "    word_freq = [(word, word_count_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    # Sort the dictionary by frequency and get top n words\n",
    "    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top n words\n",
    "    return word_freq[:n]\n",
    "\n",
    "# Get top 10 words from the preprocessed text in spam and ham messages\n",
    "spam_messages = data[data['label'] == 1]['preprocessed_text']\n",
    "ham_messages = data[data['label'] == 0]['preprocessed_text']\n",
    "\n",
    "top_10_spam_words = get_top_n_words(spam_messages)\n",
    "top_10_ham_words = get_top_n_words(ham_messages)\n",
    "\n",
    "# Display the top 10 words\n",
    "print(\"Top 10 words in spam messages:\", top_10_spam_words)\n",
    "print(\"Top 10 words in ham messages:\", top_10_ham_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salva\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'label' is your target column\n",
    "y = data['label']\n",
    "\n",
    "# Split the combined matrix into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model (logistic regression as an example)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  usiness is for the fact that the deceased man ...\n",
      "1  They are happy to adjust to the afternoon. I a...\n",
      "2  Lael Brainard was confirmed 78-19 this afterno...\n",
      "3  H <hrod17@clintonemail.com>Friday March 26 201...\n",
      "4  n;\"> Dear Good Friend,<br><br><br>I am happy t...\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "path_test = WindowsPath(\"C:/Users/Salva/Documents/AI Engeneering Ironhack/Week 4/lab-natural-language-processing/data/kg_test.csv\")\n",
    "test_data = pd.read_csv(path_test)\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps to the test data\n",
    "test_data['preprocessed_text'] = test_data['text'].apply(clean_text)\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(remove_stopwords)\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(lemmatize_text)\n",
    "\n",
    "# Add new features to the test data\n",
    "test_data['money_mark'] = test_data['preprocessed_text'].str.contains(money_simbol_list).astype(int)\n",
    "test_data['suspicious_words'] = test_data['preprocessed_text'].str.contains(suspicious_words).astype(int)\n",
    "test_data['text_len'] = test_data['preprocessed_text'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same CountVectorizer or TfidfVectorizer that was trained on the training data\n",
    "X_test_bow = vectorizer.transform(test_data['preprocessed_text'])  # Apply vectorization on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Extract the new features from the test data\n",
    "test_features = test_data[['money_mark', 'suspicious_words', 'text_len']]\n",
    "\n",
    "# Convert the new features to a sparse matrix\n",
    "test_features_sparse = csr_matrix(test_features.values)\n",
    "\n",
    "# Combine BoW/TF-IDF matrix with new features\n",
    "X_test_combined = hstack([X_test_bow, test_features_sparse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the labels for the test data\n",
    "y_test_pred = model.predict(X_test_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Salva\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming the 'label' column exists in the test data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m y_test_true \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Replace with the actual label column in the test file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate the accuracy of the model on the test data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_true, y_test_pred)\n",
      "File \u001b[1;32mc:\\Users\\Salva\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Salva\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming the 'label' column exists in the test data\n",
    "y_test_true = test_data['label']  # Replace with the actual label column in the test file\n",
    "\n",
    "# Evaluate the accuracy of the model on the test data\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print the classification report for more detailed metrics\n",
    "print(classification_report(y_test_true, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
